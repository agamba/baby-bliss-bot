{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training a Stable Diffusion Model with Texture Inversion\n",
        "\n",
        "**Objectives**: Train a diffusion Model with a few Bliss symbol images to associate them with a token `<bliss-symbol>`. The model is trained with Flax/JAX.\n",
        "\n",
        "**Parameters**: All parameters are set at step 2.2. Refer to [Example of texture inversion training with Flax/JAX](https://github.com/huggingface/diffusers/tree/main/examples/textual_inversion#training-with-flaxjax) about what they are. \n",
        "\n",
        "**Note**: If using Azure to run this job, the sensitive information for connecting to the Azure subscription in the\n",
        "section 1 needs to be filled in before running. If using other platform, this section should be replaced with the\n",
        "credential verification for that platform.\n",
        "\n",
        "**References**\n",
        "* [Using Textual Inversion Embeddings to gain substantial control over your generated images](https://blog.paperspace.com/dreambooth-stable-diffusion-tutorial-part-2-textual-inversion/)\n",
        "* [How to Fine-tune Stable Diffusion using Textual Inversion](https://towardsdatascience.com/how-to-fine-tune-stable-diffusion-using-textual-inversion-b995d7ecc095)\n",
        "* [Example of texture inversion training](https://github.com/huggingface/diffusers/tree/main/examples/textual_inversion)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Connect to Azure Machine Learning workspace\n",
        "\n",
        "Before we dive in the code, you'll need to connect to your workspace. The workspace is the top-level resource for Azure Machine Learning, providing a centralized place to work with all the artifacts you create when you use Azure Machine Learning.\n",
        "\n",
        "We are using `DefaultAzureCredential` to get access to workspace. `DefaultAzureCredential` should be capable of handling most scenarios. If you want to learn more about other available credentials, go to [set up authentication doc](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-setup-authentication?tabs=sdk), [azure-identity reference doc](https://learn.microsoft.com/en-us/python/api/azure-identity/azure.identity?view=azure-python).\n",
        "\n",
        "**Make sure to enter your workspace credentials before you run the script below.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1681152906611
        }
      },
      "outputs": [],
      "source": [
        "# Handle to the workspace\n",
        "from azure.ai.ml import MLClient\n",
        "\n",
        "# Authentication package\n",
        "from azure.identity import DefaultAzureCredential\n",
        "\n",
        "credential = DefaultAzureCredential()\n",
        "\n",
        "# Get a handle to the workspace. You can find the info on the workspace tab on ml.azure.com\n",
        "ml_client = MLClient(\n",
        "    credential=credential,\n",
        "    subscription_id=\"...\",  # this will look like xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\n",
        "    resource_group_name=\"...\",\n",
        "    workspace_name=\"...\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### 2. Settings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### 2.1 import required libs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1681152951059
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import logging\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "from pathlib import Path\n",
        "from typing import Optional\n",
        "\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import optax\n",
        "import PIL\n",
        "import torch\n",
        "import torch.utils.checkpoint\n",
        "import transformers\n",
        "from flax import jax_utils\n",
        "from flax.training import train_state\n",
        "from flax.training.common_utils import shard\n",
        "from huggingface_hub import HfFolder, Repository, create_repo, whoami\n",
        "\n",
        "# TODO: remove and import from diffusers.utils when the new version of diffusers is released\n",
        "from packaging import version\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import CLIPImageProcessor, CLIPTokenizer, FlaxCLIPTextModel, set_seed\n",
        "\n",
        "from diffusers import (\n",
        "    FlaxAutoencoderKL,\n",
        "    FlaxDDPMScheduler,\n",
        "    FlaxPNDMScheduler,\n",
        "    FlaxStableDiffusionPipeline,\n",
        "    FlaxUNet2DConditionModel,\n",
        ")\n",
        "from diffusers.pipelines.stable_diffusion import FlaxStableDiffusionSafetyChecker\n",
        "from diffusers.utils import check_min_version\n",
        "\n",
        "if version.parse(version.parse(PIL.__version__).base_version) >= version.parse(\"9.1.0\"):\n",
        "    PIL_INTERPOLATION = {\n",
        "        \"linear\": PIL.Image.Resampling.BILINEAR,\n",
        "        \"bilinear\": PIL.Image.Resampling.BILINEAR,\n",
        "        \"bicubic\": PIL.Image.Resampling.BICUBIC,\n",
        "        \"lanczos\": PIL.Image.Resampling.LANCZOS,\n",
        "        \"nearest\": PIL.Image.Resampling.NEAREST,\n",
        "    }\n",
        "else:\n",
        "    PIL_INTERPOLATION = {\n",
        "        \"linear\": PIL.Image.LINEAR,\n",
        "        \"bilinear\": PIL.Image.BILINEAR,\n",
        "        \"bicubic\": PIL.Image.BICUBIC,\n",
        "        \"lanczos\": PIL.Image.LANCZOS,\n",
        "        \"nearest\": PIL.Image.NEAREST,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### 2.2 set up constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1681154646797
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "pretrained_model_name_or_path=\"duongna/stable-diffusion-v1-4-flax\"\n",
        "train_data_dir='./data/texture_inversion'\n",
        "learnable_property=\"object\"\n",
        "placeholder_token=\"<bliss-symbol>\"\n",
        "initializer_token='bliss'\n",
        "seed=42\n",
        "resolution=128\n",
        "train_batch_size=1\n",
        "num_train_epochs=100\n",
        "max_train_steps=3000\n",
        "save_steps=500\n",
        "scale_lr=True\n",
        "learning_rate=5.0e-04\n",
        "lr_warmup_steps=500\n",
        "repeats=100\n",
        "center_crop=False\n",
        "lr_scheduler=\"constant\"\n",
        "adam_beta1=0.9\n",
        "adam_beta2=0.999\n",
        "adam_weight_decay=1e-2\n",
        "adam_epsilon=1e-08\n",
        "output_dir='./output'\n",
        "\n",
        "# Setup the prompt templates for training\n",
        "imagenet_templates_small = [\n",
        "    \"a photo of a {}\",\n",
        "    \"a rendering of a {}\",\n",
        "    \"a cropped photo of the {}\",\n",
        "    \"the photo of a {}\",\n",
        "    \"a photo of a clean {}\",\n",
        "    \"a photo of a dirty {}\",\n",
        "    \"a dark photo of the {}\",\n",
        "    \"a photo of my {}\",\n",
        "    \"a photo of the cool {}\",\n",
        "    \"a close-up photo of a {}\",\n",
        "    \"a bright photo of the {}\",\n",
        "    \"a cropped photo of a {}\",\n",
        "    \"a photo of the {}\",\n",
        "    \"a good photo of the {}\",\n",
        "    \"a photo of one {}\",\n",
        "    \"a close-up photo of the {}\",\n",
        "    \"a rendition of the {}\",\n",
        "    \"a photo of the clean {}\",\n",
        "    \"a rendition of a {}\",\n",
        "    \"a photo of a nice {}\",\n",
        "    \"a good photo of a {}\",\n",
        "    \"a photo of the nice {}\",\n",
        "    \"a photo of the small {}\",\n",
        "    \"a photo of the weird {}\",\n",
        "    \"a photo of the large {}\",\n",
        "    \"a photo of a cool {}\",\n",
        "    \"a photo of a small {}\",\n",
        "]\n",
        "\n",
        "imagenet_style_templates_small = [\n",
        "    \"a painting in the style of {}\",\n",
        "    \"a rendering in the style of {}\",\n",
        "    \"a cropped painting in the style of {}\",\n",
        "    \"the painting in the style of {}\",\n",
        "    \"a clean painting in the style of {}\",\n",
        "    \"a dirty painting in the style of {}\",\n",
        "    \"a dark painting in the style of {}\",\n",
        "    \"a picture in the style of {}\",\n",
        "    \"a cool painting in the style of {}\",\n",
        "    \"a close-up painting in the style of {}\",\n",
        "    \"a bright painting in the style of {}\",\n",
        "    \"a cropped painting in the style of {}\",\n",
        "    \"a good painting in the style of {}\",\n",
        "    \"a close-up painting in the style of {}\",\n",
        "    \"a rendition in the style of {}\",\n",
        "    \"a nice painting in the style of {}\",\n",
        "    \"a small painting in the style of {}\",\n",
        "    \"a weird painting in the style of {}\",\n",
        "    \"a large painting in the style of {}\",\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### 2.3 set up helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1681154774050
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "class TextualInversionDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        data_root,\n",
        "        tokenizer,\n",
        "        learnable_property=\"object\",  # [object, style]\n",
        "        size=512,\n",
        "        repeats=100,\n",
        "        interpolation=\"bicubic\",\n",
        "        flip_p=0.5,\n",
        "        set=\"train\",\n",
        "        placeholder_token=\"*\",\n",
        "        center_crop=False,\n",
        "    ):\n",
        "        self.data_root = data_root\n",
        "        self.tokenizer = tokenizer\n",
        "        self.learnable_property = learnable_property\n",
        "        self.size = size\n",
        "        self.placeholder_token = placeholder_token\n",
        "        self.center_crop = center_crop\n",
        "        self.flip_p = flip_p\n",
        "\n",
        "        self.image_paths = [os.path.join(self.data_root, file_path) for file_path in os.listdir(self.data_root) if file_path.endswith('.png')]\n",
        "        print(\"self.image_paths: \", self.image_paths)\n",
        "\n",
        "        self.num_images = len(self.image_paths)\n",
        "        self._length = self.num_images\n",
        "\n",
        "        if set == \"train\":\n",
        "            self._length = self.num_images * repeats\n",
        "\n",
        "        self.interpolation = {\n",
        "            \"linear\": PIL_INTERPOLATION[\"linear\"],\n",
        "            \"bilinear\": PIL_INTERPOLATION[\"bilinear\"],\n",
        "            \"bicubic\": PIL_INTERPOLATION[\"bicubic\"],\n",
        "            \"lanczos\": PIL_INTERPOLATION[\"lanczos\"],\n",
        "        }[interpolation]\n",
        "\n",
        "        self.templates = imagenet_style_templates_small if learnable_property == \"style\" else imagenet_templates_small\n",
        "        self.flip_transform = transforms.RandomHorizontalFlip(p=self.flip_p)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._length\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        example = {}\n",
        "        image = Image.open(self.image_paths[i % self.num_images])\n",
        "\n",
        "        if not image.mode == \"RGB\":\n",
        "            image = image.convert(\"RGB\")\n",
        "\n",
        "        placeholder_string = self.placeholder_token\n",
        "        text = random.choice(self.templates).format(placeholder_string)\n",
        "\n",
        "        example[\"input_ids\"] = self.tokenizer(\n",
        "            text,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.tokenizer.model_max_length,\n",
        "            return_tensors=\"pt\",\n",
        "        ).input_ids[0]\n",
        "\n",
        "        # default to score-sde preprocessing\n",
        "        img = np.array(image).astype(np.uint8)\n",
        "\n",
        "        if self.center_crop:\n",
        "            crop = min(img.shape[0], img.shape[1])\n",
        "            (\n",
        "                h,\n",
        "                w,\n",
        "            ) = (\n",
        "                img.shape[0],\n",
        "                img.shape[1],\n",
        "            )\n",
        "            img = img[(h - crop) // 2 : (h + crop) // 2, (w - crop) // 2 : (w + crop) // 2]\n",
        "\n",
        "        image = Image.fromarray(img)\n",
        "        image = image.resize((self.size, self.size), resample=self.interpolation)\n",
        "\n",
        "        image = self.flip_transform(image)\n",
        "        image = np.array(image).astype(np.uint8)\n",
        "        image = (image / 127.5 - 1.0).astype(np.float32)\n",
        "\n",
        "        example[\"pixel_values\"] = torch.from_numpy(image).permute(2, 0, 1)\n",
        "        return example\n",
        "\n",
        "def resize_token_embeddings(model, new_num_tokens, initializer_token_id, placeholder_token_id, rng):\n",
        "    if model.config.vocab_size == new_num_tokens or new_num_tokens is None:\n",
        "        return\n",
        "    model.config.vocab_size = new_num_tokens\n",
        "\n",
        "    params = model.params\n",
        "    old_embeddings = params[\"text_model\"][\"embeddings\"][\"token_embedding\"][\"embedding\"]\n",
        "    old_num_tokens, emb_dim = old_embeddings.shape\n",
        "\n",
        "    initializer = jax.nn.initializers.normal()\n",
        "\n",
        "    new_embeddings = initializer(rng, (new_num_tokens, emb_dim))\n",
        "    new_embeddings = new_embeddings.at[:old_num_tokens].set(old_embeddings)\n",
        "    new_embeddings = new_embeddings.at[placeholder_token_id].set(new_embeddings[initializer_token_id])\n",
        "    params[\"text_model\"][\"embeddings\"][\"token_embedding\"][\"embedding\"] = new_embeddings\n",
        "\n",
        "    model.params = params\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_params_to_save(params):\n",
        "    return jax.device_get(jax.tree_util.tree_map(lambda x: x[0], params))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### 3 Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### 3.1 load tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1681153163608
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Load the tokenizer and add the placeholder token as a additional special token\n",
        "tokenizer = CLIPTokenizer.from_pretrained(pretrained_model_name_or_path, subfolder=\"tokenizer\")\n",
        "\n",
        "# Add the placeholder token in tokenizer\n",
        "num_added_tokens = tokenizer.add_tokens(placeholder_token)\n",
        "\n",
        "print('num_added_tokens: ', num_added_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### 3.2 Convert the initializer_token, placeholder_token to ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1681153337886
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Convert the initializer_token, placeholder_token to ids\n",
        "token_ids = tokenizer.encode(initializer_token, add_special_tokens=False)\n",
        "print(\"token_ids: \", token_ids)\n",
        "\n",
        "# Check if initializer_token is a single token or a sequence of tokens\n",
        "if len(token_ids) > 1:\n",
        "    raise ValueError(\"The initializer token must be a single token.\")\n",
        "\n",
        "initializer_token_id = token_ids[0]\n",
        "placeholder_token_id = tokenizer.convert_tokens_to_ids(placeholder_token)\n",
        "\n",
        "print(\"initializer_token_id: \", initializer_token_id)\n",
        "print(\"placeholder_token_id: \", placeholder_token_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### 3.3 load models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1681154824633
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Load models and create wrapper for stable diffusion\n",
        "text_encoder = FlaxCLIPTextModel.from_pretrained(\n",
        "    pretrained_model_name_or_path, subfolder=\"text_encoder\"\n",
        ")\n",
        "vae, vae_params = FlaxAutoencoderKL.from_pretrained(\n",
        "    pretrained_model_name_or_path, subfolder=\"vae\"\n",
        ")\n",
        "unet, unet_params = FlaxUNet2DConditionModel.from_pretrained(\n",
        "    pretrained_model_name_or_path, subfolder=\"unet\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### 3.4 Create sampling RNG and train dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1681154834840
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Create sampling rng\n",
        "rng = jax.random.PRNGKey(seed)\n",
        "rng, _ = jax.random.split(rng)\n",
        "# Resize the token embeddings as we are adding new special tokens to the tokenizer\n",
        "text_encoder = resize_token_embeddings(\n",
        "    text_encoder, len(tokenizer), initializer_token_id, placeholder_token_id, rng\n",
        ")\n",
        "original_token_embeds = text_encoder.params[\"text_model\"][\"embeddings\"][\"token_embedding\"][\"embedding\"]\n",
        "\n",
        "train_dataset = TextualInversionDataset(\n",
        "    data_root=train_data_dir,\n",
        "    tokenizer=tokenizer,\n",
        "    size=resolution,\n",
        "    placeholder_token=placeholder_token,\n",
        "    repeats=repeats,\n",
        "    learnable_property=learnable_property,\n",
        "    center_crop=center_crop,\n",
        "    set=\"train\",\n",
        ")\n",
        "\n",
        "def collate_fn(examples):\n",
        "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
        "    input_ids = torch.stack([example[\"input_ids\"] for example in examples])\n",
        "\n",
        "    batch = {\"pixel_values\": pixel_values, \"input_ids\": input_ids}\n",
        "    batch = {k: v.numpy() for k, v in batch.items()}\n",
        "\n",
        "    return batch\n",
        "\n",
        "total_train_batch_size = train_batch_size * jax.local_device_count()\n",
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=total_train_batch_size, shuffle=True, drop_last=True, collate_fn=collate_fn\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### 3.5 Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1681154843536
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "learning_rate = learning_rate * total_train_batch_size\n",
        "\n",
        "constant_scheduler = optax.constant_schedule(learning_rate)\n",
        "\n",
        "optimizer = optax.adamw(\n",
        "    learning_rate=constant_scheduler,\n",
        "    b1=adam_beta1,\n",
        "    b2=adam_beta2,\n",
        "    eps=adam_epsilon,\n",
        "    weight_decay=adam_weight_decay,\n",
        ")\n",
        "\n",
        "def create_mask(params, label_fn):\n",
        "    def _map(params, mask, label_fn):\n",
        "        for k in params:\n",
        "            if label_fn(k):\n",
        "                mask[k] = \"token_embedding\"\n",
        "            else:\n",
        "                if isinstance(params[k], dict):\n",
        "                    mask[k] = {}\n",
        "                    _map(params[k], mask[k], label_fn)\n",
        "                else:\n",
        "                    mask[k] = \"zero\"\n",
        "\n",
        "    mask = {}\n",
        "    _map(params, mask, label_fn)\n",
        "    return mask\n",
        "\n",
        "def zero_grads():\n",
        "    # from https://github.com/deepmind/optax/issues/159#issuecomment-896459491\n",
        "    def init_fn(_):\n",
        "        return ()\n",
        "\n",
        "    def update_fn(updates, state, params=None):\n",
        "        return jax.tree_util.tree_map(jnp.zeros_like, updates), ()\n",
        "\n",
        "    return optax.GradientTransformation(init_fn, update_fn)\n",
        "\n",
        "# Zero out gradients of layers other than the token embedding layer\n",
        "tx = optax.multi_transform(\n",
        "    {\"token_embedding\": optimizer, \"zero\": zero_grads()},\n",
        "    create_mask(text_encoder.params, lambda s: s == \"token_embedding\"),\n",
        ")\n",
        "\n",
        "state = train_state.TrainState.create(apply_fn=text_encoder.__call__, params=text_encoder.params, tx=tx)\n",
        "\n",
        "noise_scheduler = FlaxDDPMScheduler(\n",
        "    beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000\n",
        ")\n",
        "noise_scheduler_state = noise_scheduler.create_state()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### 3.6 Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1681154280849
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Initialize our training\n",
        "train_rngs = jax.random.split(rng, jax.local_device_count())\n",
        "\n",
        "# Define gradient train step fn\n",
        "def train_step(state, vae_params, unet_params, batch, train_rng):\n",
        "    dropout_rng, sample_rng, new_train_rng = jax.random.split(train_rng, 3)\n",
        "\n",
        "    def compute_loss(params):\n",
        "        vae_outputs = vae.apply(\n",
        "            {\"params\": vae_params}, batch[\"pixel_values\"], deterministic=True, method=vae.encode\n",
        "        )\n",
        "        latents = vae_outputs.latent_dist.sample(sample_rng)\n",
        "        # (NHWC) -> (NCHW)\n",
        "        latents = jnp.transpose(latents, (0, 3, 1, 2))\n",
        "        latents = latents * vae.config.scaling_factor\n",
        "\n",
        "        noise_rng, timestep_rng = jax.random.split(sample_rng)\n",
        "        noise = jax.random.normal(noise_rng, latents.shape)\n",
        "        bsz = latents.shape[0]\n",
        "        timesteps = jax.random.randint(\n",
        "            timestep_rng,\n",
        "            (bsz,),\n",
        "            0,\n",
        "            noise_scheduler.config.num_train_timesteps,\n",
        "        )\n",
        "        noisy_latents = noise_scheduler.add_noise(noise_scheduler_state, latents, noise, timesteps)\n",
        "        encoder_hidden_states = state.apply_fn(\n",
        "            batch[\"input_ids\"], params=params, dropout_rng=dropout_rng, train=True\n",
        "        )[0]\n",
        "        # Predict the noise residual and compute loss\n",
        "        model_pred = unet.apply(\n",
        "            {\"params\": unet_params}, noisy_latents, timesteps, encoder_hidden_states, train=False\n",
        "        ).sample\n",
        "\n",
        "        # Get the target for loss depending on the prediction type\n",
        "        if noise_scheduler.config.prediction_type == \"epsilon\":\n",
        "            target = noise\n",
        "        elif noise_scheduler.config.prediction_type == \"v_prediction\":\n",
        "            target = noise_scheduler.get_velocity(noise_scheduler_state, latents, noise, timesteps)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown prediction type {noise_scheduler.config.prediction_type}\")\n",
        "\n",
        "        loss = (target - model_pred) ** 2\n",
        "        loss = loss.mean()\n",
        "\n",
        "        return loss\n",
        "\n",
        "    grad_fn = jax.value_and_grad(compute_loss)\n",
        "    loss, grad = grad_fn(state.params)\n",
        "    grad = jax.lax.pmean(grad, \"batch\")\n",
        "    new_state = state.apply_gradients(grads=grad)\n",
        "\n",
        "    # Keep the token embeddings fixed except the newly added embeddings for the concept,\n",
        "    # as we only want to optimize the concept embeddings\n",
        "    token_embeds = original_token_embeds.at[placeholder_token_id].set(\n",
        "        new_state.params[\"text_model\"][\"embeddings\"][\"token_embedding\"][\"embedding\"][placeholder_token_id]\n",
        "    )\n",
        "    new_state.params[\"text_model\"][\"embeddings\"][\"token_embedding\"][\"embedding\"] = token_embeds\n",
        "\n",
        "    metrics = {\"loss\": loss}\n",
        "    metrics = jax.lax.pmean(metrics, axis_name=\"batch\")\n",
        "    return new_state, metrics, new_train_rng\n",
        "\n",
        "# Create parallel version of the train and eval step\n",
        "p_train_step = jax.pmap(train_step, \"batch\", donate_argnums=(0,))\n",
        "\n",
        "# Replicate the train state on each device\n",
        "state = jax_utils.replicate(state)\n",
        "vae_params = jax_utils.replicate(vae_params)\n",
        "unet_params = jax_utils.replicate(unet_params)\n",
        "\n",
        "# Train!\n",
        "num_update_steps_per_epoch = math.ceil(len(train_dataloader))\n",
        "\n",
        "# Scheduler and math around the number of training steps.\n",
        "if max_train_steps is None:\n",
        "    max_train_steps = num_train_epochs * num_update_steps_per_epoch\n",
        "\n",
        "num_train_epochs = math.ceil(max_train_steps / num_update_steps_per_epoch)\n",
        "\n",
        "print(\"***** Running training *****\")\n",
        "print(f\"  Num examples = {len(train_dataset)}\")\n",
        "print(f\"  Num Epochs = {num_train_epochs}\")\n",
        "print(f\"  Instantaneous batch size per device = {train_batch_size}\")\n",
        "print(f\"  Total train batch size (w. parallel & distributed) = {total_train_batch_size}\")\n",
        "print(f\"  Total optimization steps = {max_train_steps}\")\n",
        "\n",
        "global_step = 0\n",
        "\n",
        "epochs = tqdm(range(num_train_epochs), desc=f\"Epoch ... (1/{num_train_epochs})\", position=0)\n",
        "for epoch in epochs:\n",
        "    # ======================== Training ================================\n",
        "\n",
        "    train_metrics = []\n",
        "\n",
        "    steps_per_epoch = len(train_dataset) // total_train_batch_size\n",
        "    train_step_progress_bar = tqdm(total=steps_per_epoch, desc=\"Training...\", position=1, leave=False)\n",
        "    # train\n",
        "    for batch in train_dataloader:\n",
        "        batch = shard(batch)\n",
        "        state, train_metric, train_rngs = p_train_step(state, vae_params, unet_params, batch, train_rngs)\n",
        "        train_metrics.append(train_metric)\n",
        "\n",
        "        train_step_progress_bar.update(1)\n",
        "        global_step += 1\n",
        "\n",
        "        if global_step >= max_train_steps:\n",
        "            break\n",
        "        if global_step % save_steps == 0:\n",
        "            learned_embeds = get_params_to_save(state.params)[\"text_model\"][\"embeddings\"][\"token_embedding\"][\n",
        "                \"embedding\"\n",
        "            ][placeholder_token_id]\n",
        "            learned_embeds_dict = {placeholder_token: learned_embeds}\n",
        "            jnp.save(\n",
        "                os.path.join(output_dir, \"learned_embeds-\" + str(global_step) + \".npy\"), learned_embeds_dict\n",
        "            )\n",
        "\n",
        "    train_metric = jax_utils.unreplicate(train_metric)\n",
        "\n",
        "    train_step_progress_bar.close()\n",
        "    epochs.write(f\"Epoch... ({epoch + 1}/{num_train_epochs} | Loss: {train_metric['loss']})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### 3.7 Create the pipeline using the trained modules and save it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1681179628520
        }
      },
      "outputs": [],
      "source": [
        "if jax.process_index() == 0:\n",
        "    scheduler = FlaxPNDMScheduler(\n",
        "        beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", skip_prk_steps=True\n",
        "    )\n",
        "    safety_checker = FlaxStableDiffusionSafetyChecker.from_pretrained(\n",
        "        \"CompVis/stable-diffusion-safety-checker\", from_pt=True\n",
        "    )\n",
        "    pipeline = FlaxStableDiffusionPipeline(\n",
        "        text_encoder=text_encoder,\n",
        "        vae=vae,\n",
        "        unet=unet,\n",
        "        tokenizer=tokenizer,\n",
        "        scheduler=scheduler,\n",
        "        safety_checker=safety_checker,\n",
        "        feature_extractor=CLIPImageProcessor.from_pretrained(\"openai/clip-vit-base-patch32\"),\n",
        "    )\n",
        "\n",
        "    pipeline.save_pretrained(\n",
        "        output_dir,\n",
        "        params={\n",
        "            \"text_encoder\": get_params_to_save(state.params),\n",
        "            \"vae\": get_params_to_save(vae_params),\n",
        "            \"unet\": get_params_to_save(unet_params),\n",
        "            \"safety_checker\": safety_checker.params,\n",
        "        },\n",
        "    )\n",
        "\n",
        "    # Also save the newly trained embeddings\n",
        "    learned_embeds = get_params_to_save(state.params)[\"text_model\"][\"embeddings\"][\"token_embedding\"][\"embedding\"][\n",
        "        placeholder_token_id\n",
        "    ]\n",
        "    learned_embeds_dict = {placeholder_token: learned_embeds}\n",
        "    jnp.save(os.path.join(output_dir, \"flax_learned_embeds.npy\"), learned_embeds_dict)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### 4. Use the fine-tuned model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1681181234562
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from diffusers import StableDiffusionPipeline\n",
        "\n",
        "model_id = \"./output\"\n",
        "pipe = StableDiffusionPipeline.from_pretrained(model_id,from_flax=True,safety_checker=None)\n",
        "\n",
        "# prompt = \"A <bliss-symbol> on a backpack\"\n",
        "prompt = \"A new <bliss-symbol> for microcosm\"\n",
        "\n",
        "image = pipe(prompt, num_inference_steps=50, guidance_scale=7.5).images[0]\n",
        "\n",
        "# image.save(\"./data/texture_inversion_fineTuned_results/bliss-on-backpack.png\")\n",
        "image.save(\"./data/texture_inversion_fineTuned_results/bliss-for-microcosm.png\")\n"
      ]
    }
  ],
  "metadata": {
    "categories": [
      "SDK v2",
      "tutorials"
    ],
    "description": {
      "description": "A quickstart tutorial to train and deploy an image classification model on Azure Machine Learning studio"
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "Python 3.10 - SDK v2",
      "language": "python",
      "name": "python310-sdkv2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "ad53975d6cd14b07e2ef76ca5c680233933f2b5b5c4b5fa1fc47a72f4636b78d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
